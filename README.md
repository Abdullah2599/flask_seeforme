# üöÄ Advanced Multi-Dataset Object Detection System

A comprehensive YOLOv8-based object detection system trained on multiple datasets with **1500+ classes**, specifically enhanced for **Pakistan/South Asian objects**. Built for visually impaired users with Flask backend integration.

![Python](https://img.shields.io/badge/python-v3.8+-blue.svg)
![YOLOv8](https://img.shields.io/badge/YOLOv8-latest-green.svg)
![Flask](https://img.shields.io/badge/Flask-2.3.3-red.svg)
![License](https://img.shields.io/badge/license-MIT-blue.svg)

## üåü Features

### üéØ **Advanced Object Detection**
- **1500+ Object Classes**: Trained on COCO, Open Images V7, LVIS, Objects365, and ImageNet Detection
- **Pakistan-Specific Objects**: 80+ regional objects (currency, food, clothing, transportation, etc.)
- **High Accuracy**: >0.5 mAP@0.5 on merged validation set
- **Fast Inference**: <100ms on GPU, <500ms on CPU

### üß† **Smart Model Management**
- **Automatic Model Selection**: Chooses best available model based on performance metrics
- **Model Versioning**: Complete version control with metadata tracking
- **Inference Caching**: Smart caching for similar frames in real-time video
- **GPU/CPU Optimization**: Automatic device selection and memory management

### üáµüá∞ **Regional Enhancement**
- **Pakistan-Specific Classes**: Currency, food items, clothing, transportation, religious items
- **Cultural Context**: Optimized for South Asian environments and lighting conditions
- **Local Language Support**: Urdu script recognition capabilities
- **Market Integration**: Street vendors, local transportation, traditional items

### üîß **Production-Ready Backend**
- **Enhanced Flask API**: Multiple endpoints for model management and health monitoring
- **Backward Compatibility**: Maintains original API structure
- **Error Handling**: Comprehensive error handling and logging
- **Performance Monitoring**: Real-time inference statistics and caching metrics

## üìÅ Project Structure

```
flask_seeforme/
‚îú‚îÄ‚îÄ üìä data_pipeline/              # Dataset processing pipeline
‚îÇ   ‚îú‚îÄ‚îÄ download_datasets.py       # Automated dataset downloads
‚îÇ   ‚îú‚îÄ‚îÄ convert_to_yolo.py        # Format conversion to YOLO
‚îÇ   ‚îî‚îÄ‚îÄ merge_datasets.py         # Intelligent dataset merging
‚îú‚îÄ‚îÄ üöÇ training/                   # Model training pipeline
‚îÇ   ‚îú‚îÄ‚îÄ train_custom_model.py     # YOLOv8x training script
‚îÇ   ‚îú‚îÄ‚îÄ evaluation.py             # Comprehensive evaluation
‚îÇ   ‚îî‚îÄ‚îÄ hyperparameter_config.yaml # Training configuration
‚îú‚îÄ‚îÄ üáµüá∞ regional_data/             # Pakistan-specific data
‚îÇ   ‚îú‚îÄ‚îÄ collect_pakistan_data.py  # Web scraping for regional objects
‚îÇ   ‚îú‚îÄ‚îÄ pseudo_label.py           # Automatic annotation with CLIP
‚îÇ   ‚îî‚îÄ‚îÄ pakistan_classes.txt      # Regional object classes
‚îú‚îÄ‚îÄ üåê flask_backend/              # Enhanced backend
‚îÇ   ‚îî‚îÄ‚îÄ model_manager.py          # Smart model management
‚îú‚îÄ‚îÄ üìì notebooks/                  # Jupyter notebooks
‚îÇ   ‚îî‚îÄ‚îÄ training_colab.ipynb      # Complete Colab training pipeline
‚îú‚îÄ‚îÄ üèóÔ∏è models/                     # Trained models storage
‚îú‚îÄ‚îÄ üìä datasets/                   # Raw datasets
‚îú‚îÄ‚îÄ üîÑ merged_dataset/             # Processed unified dataset
‚îú‚îÄ‚îÄ üìà evaluation_results/         # Model evaluation outputs
‚îú‚îÄ‚îÄ app.py                        # Enhanced Flask application
‚îú‚îÄ‚îÄ requirements.txt              # Python dependencies
‚îú‚îÄ‚îÄ setup.sh                     # Automated setup script
‚îî‚îÄ‚îÄ README.md                    # This file
```

## üöÄ Quick Start

### 1. **Automated Setup**
```bash
# Clone the repository
git clone https://github.com/Abdullah2599/flask_seeforme.git
cd flask_seeforme

# Run automated setup
chmod +x setup.sh
./setup.sh
```

### 2. **Start Flask Backend**
```bash
# Development server
./start_flask_dev.sh

# Production server
./start_flask_prod.sh
```

### 3. **Run Complete Training Pipeline**
```bash
# Full pipeline: download ‚Üí process ‚Üí train ‚Üí evaluate
./run_training_pipeline.sh
```

### 4. **Use Google Colab for Training**
- Open `notebooks/training_colab.ipynb` in Google Colab
- Ensure GPU runtime is enabled
- Run all cells for complete training pipeline

## üîå API Endpoints

### **Core Detection**
- `POST /predict` - Object detection on uploaded image
- `GET /` - Health check

### **Model Management**
- `GET /model/info` - Current model information and capabilities
- `GET /model/classes` - List of supported object classes
- `POST /model/switch` - Switch between available models

### **Performance Monitoring**
- `GET /cache/stats` - Inference cache statistics
- `POST /cache/clear` - Clear inference cache
- `GET /health` - Detailed system health check

### **API Usage Example**
```python
import requests

# Object detection
with open('image.jpg', 'rb') as f:
    response = requests.post('http://localhost:5000/predict', 
                           files={'image': f})
    
detections = response.json()
print(f"Found {detections['count']} objects:")
for detection in detections['detections']:
    print(f"- {detection['label']}: {detection['confidence']:.3f}")
```

## üìä Model Performance

### **Overall Metrics**
- **mAP@0.5**: >0.5 on merged validation set
- **mAP@0.5:0.95**: >0.3 on merged validation set
- **Total Classes**: 1500-2000+ unified object categories
- **Model Size**: <500MB for deployment optimization

### **Pakistan-Specific Performance**
- **Regional Classes**: 80+ Pakistan/South Asian objects
- **Cultural Accuracy**: Optimized for local contexts
- **Lighting Conditions**: Robust performance in various lighting
- **Real-world Testing**: Validated on Pakistani street scenes

### **Inference Performance**
- **GPU Inference**: <100ms per image
- **CPU Inference**: <500ms per image
- **Memory Usage**: <2GB RAM for inference
- **Batch Processing**: Supports multiple images

## üéØ Supported Object Categories

### **Global Objects (1400+ classes)**
- **COCO**: 80 common objects (person, car, chair, etc.)
- **Open Images V7**: 600 diverse everyday objects
- **LVIS**: 1200+ long-tail objects
- **Objects365**: 365 additional object categories
- **ImageNet Detection**: 200 supplementary classes

### **Pakistan-Specific Objects (80+ classes)**

#### üí∞ **Currency**
- Pakistani rupee notes (1000, 500, 100, 50, 20, 10, 5)
- Rupee coins (1, 2, 5, 10)

#### üçõ **Food Items**
- Roti, naan, biryani, karahi, chai cup
- Samosa, pakora, kebab, daal, halwa
- Lassi, kulfi, paratha, qorma, nihari

#### üëï **Clothing**
- Shalwar kameez, dupatta, kurta, sherwani
- Lehenga, churidar, pagri turban, topi cap
- Khussay shoes, chappal sandals

#### üöó **Transportation**
- Rickshaw, chingchi, suzuki van, local bus
- Motorcycle 70cc, bicycle, tanga horse cart
- Truck art (distinctive Pakistani truck decorations)

#### üè† **Household Items**
- Hookah, surahi water pot, tawa griddle
- Charpoy bed, matka water pot, thali plate
- Lota water vessel, chimta tongs

#### üïå **Religious Items**
- Prayer mat (janamaz), tasbih beads, Quran
- Mosque minaret, eid decorations, prayer cap
- Islamic calligraphy, qibla compass

## üîß Training Pipeline

### **Dataset Processing**
1. **Automated Download**: COCO, Open Images V7, LVIS, Objects365
2. **Format Conversion**: Convert all annotations to YOLO format
3. **Intelligent Merging**: Handle class conflicts and create unified vocabulary
4. **Regional Data Collection**: Web scraping for Pakistan-specific objects
5. **Pseudo-Labeling**: Automatic annotation using CLIP and BLIP models

### **Model Training**
- **Architecture**: YOLOv8x (largest variant for best accuracy)
- **Transfer Learning**: Start from COCO-pretrained weights
- **Optimization**: AdamW optimizer with cosine learning rate scheduling
- **Augmentation**: Advanced data augmentation for real-world conditions
- **Hardware**: Optimized for Google Colab T4/V100 GPUs

### **Training Configuration**
```yaml
epochs: 300
batch_size: auto  # Based on GPU memory
image_size: 640
optimizer: AdamW
learning_rate: 0.01
patience: 50  # Early stopping
augmentation: advanced  # Multi-scale, mosaic, mixup
```

## üìà Evaluation and Quality Assurance

### **Comprehensive Metrics**
- **mAP Scores**: @0.5 and @0.5:0.95
- **Per-Class Analysis**: Individual class performance
- **Confusion Matrix**: Top 100 classes visualization
- **Pakistan-Specific Metrics**: Regional object performance

### **Visualization Dashboard**
- **Performance Charts**: Precision vs Recall, F1 distribution
- **Sample Predictions**: Visual validation of model outputs
- **Error Analysis**: False positive/negative identification
- **Regional Performance**: Pakistan-specific object analysis

### **Quality Assurance**
- **Automated Testing**: Unit tests for all pipeline components
- **Performance Benchmarks**: Speed and accuracy validation
- **Real-world Testing**: Validation on Pakistani street scenes
- **Edge Case Handling**: Robust performance in challenging conditions

## üöÄ Deployment Options

### **Development**
```bash
# Local development server
python app.py
# or
./start_flask_dev.sh
```

### **Production**
```bash
# Production server with Gunicorn
./start_flask_prod.sh
```

### **Cloud Deployment**

#### **Railway**
1. Connect GitHub repository
2. Set environment variables
3. Deploy automatically

#### **Render**
1. Connect repository
2. Configure build settings
3. Deploy with auto-scaling

#### **Google Cloud Run**
```dockerfile
FROM python:3.8-slim
COPY . /app
WORKDIR /app
RUN pip install -r requirements.txt
CMD ["python", "app.py"]
```

### **Docker Deployment**
```bash
# Build image
docker build -t yolov8-detection .

# Run container
docker run -p 5000:5000 yolov8-detection
```

## üîß Configuration

### **Environment Variables**
```bash
# .env file
FLASK_ENV=production
FLASK_DEBUG=False
MODEL_DIR=models
CACHE_SIZE=100
INFERENCE_TIMEOUT=30
MAX_IMAGE_SIZE=2048
MIN_CONFIDENCE=0.25
```

### **Model Configuration**
- **Default Model**: YOLOv8n (for fast startup)
- **Custom Models**: Place `.pt` files in `models/` directory
- **Auto-Selection**: Best model chosen based on performance metrics
- **Fallback**: Automatic fallback to default model if custom model fails

## üìù Usage Examples

### **Python Client**
```python
import requests
from PIL import Image

# Load and send image
with open('test_image.jpg', 'rb') as f:
    response = requests.post(
        'http://localhost:5000/predict',
        files={'image': f}
    )

# Process results
if response.json()['success']:
    detections = response.json()['detections']
    for detection in detections:
        print(f"Object: {detection['label']}")
        print(f"Confidence: {detection['confidence']:.3f}")
        print(f"Box: {detection['box']}")
```

### **Flutter Integration**
```dart
// Flutter HTTP request
final request = http.MultipartRequest(
  'POST',
  Uri.parse('http://your-server.com/predict'),
);
request.files.add(await http.MultipartFile.fromPath('image', imagePath));

final response = await request.send();
final responseData = await response.stream.bytesToString();
final detections = json.decode(responseData);
```

### **JavaScript/React**
```javascript
const formData = new FormData();
formData.append('image', imageFile);

fetch('/predict', {
  method: 'POST',
  body: formData
})
.then(response => response.json())
.then(data => {
  console.log(`Found ${data.count} objects:`, data.detections);
});
```

## üõ†Ô∏è Development

### **Setup Development Environment**
```bash
# Clone repository
git clone https://github.com/your-username/flask_seeforme.git
cd flask_seeforme

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# or
venv\Scripts\activate  # Windows

# Install dependencies
pip install -r requirements.txt

# Run tests
python -m pytest tests/

# Start development server
python app.py
```

### **Adding New Models**
1. Place model file (`.pt`) in `models/` directory
2. Create metadata file (`.json`) with same name
3. Restart Flask application
4. Model will be automatically detected and evaluated

### **Adding New Classes**
1. Update training data with new class annotations
2. Retrain model with updated dataset
3. Update class mapping files
4. Deploy new model

## üìä Performance Optimization

### **Inference Optimization**
- **Model Quantization**: Reduce model size for faster inference
- **TensorRT**: GPU acceleration for NVIDIA hardware
- **ONNX Export**: Cross-platform optimization
- **Batch Processing**: Process multiple images efficiently

### **Memory Management**
- **Smart Caching**: LRU cache for frequent predictions
- **Memory Monitoring**: Automatic cleanup when memory is low
- **GPU Memory**: Efficient CUDA memory management
- **Model Loading**: Lazy loading of large models

### **Scalability**
- **Load Balancing**: Multiple worker processes
- **Auto-scaling**: Dynamic resource allocation
- **Caching Layer**: Redis for distributed caching
- **CDN Integration**: Fast model and asset delivery

## üß™ Testing

### **Unit Tests**
```bash
# Run all tests
python -m pytest tests/

# Run specific test category
python -m pytest tests/test_model_manager.py
python -m pytest tests/test_data_pipeline.py
python -m pytest tests/test_api_endpoints.py
```

### **Integration Tests**
- **End-to-end Pipeline**: Complete training and inference
- **API Testing**: All endpoints with various inputs
- **Performance Testing**: Load testing with multiple requests
- **Error Handling**: Edge cases and error conditions

### **Manual Testing**
- **Real Images**: Test with actual Pakistani street scenes
- **Edge Cases**: Low light, blurry, occluded objects
- **Performance**: Measure inference times and accuracy
- **User Acceptance**: Testing with visually impaired users

## ü§ù Contributing

### **Development Workflow**
1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Make changes and add tests
4. Commit changes (`git commit -m 'Add amazing feature'`)
5. Push to branch (`git push origin feature/amazing-feature`)
6. Open Pull Request

### **Code Standards**
- **Python**: Follow PEP 8 style guide
- **Documentation**: Comprehensive docstrings
- **Testing**: Minimum 80% code coverage
- **Type Hints**: Use type annotations

### **Areas for Contribution**
- **New Regional Data**: Add objects from other South Asian countries
- **Model Optimization**: Improve inference speed and accuracy
- **UI/UX**: Better visualization and user interfaces
- **Documentation**: Tutorials and examples
- **Testing**: Additional test cases and scenarios

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üôè Acknowledgments

- **Ultralytics**: YOLOv8 implementation and training framework
- **OpenAI**: CLIP model for pseudo-labeling
- **Google**: Colab platform for training infrastructure
- **Dataset Providers**: COCO, Open Images, LVIS, Objects365 teams
- **Community**: Contributors and testers

## üìû Support

### **Documentation**
- **API Reference**: Detailed endpoint documentation
- **Training Guide**: Step-by-step training instructions
- **Deployment Guide**: Production deployment best practices
- **Troubleshooting**: Common issues and solutions

### **Community**
- **GitHub Issues**: Bug reports and feature requests
- **Discussions**: Community support and questions
- **Wiki**: Additional documentation and tutorials

### **Contact**
- **Email**: your-email@example.com
- **GitHub**: [@your-username](https://github.com/your-username)
- **LinkedIn**: [Your Name](https://linkedin.com/in/your-profile)

---

**Made with ‚ù§Ô∏è for the visually impaired community in Pakistan**

*This system aims to provide better accessibility and independence for visually impaired users by accurately detecting and describing objects in their environment, with special attention to culturally relevant items and contexts.*
